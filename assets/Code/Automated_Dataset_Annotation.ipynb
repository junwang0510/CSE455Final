{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a8-S5Ao3ImnT",
        "s3VuH-MxY5ls",
        "ByAcxA43a484",
        "9GaaZzF0jseo",
        "1euQGJ3ZG32x",
        "TsqhBBWVuSTR",
        "traaYMeAuara",
        "ghegsbVLvNhm"
      ],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Dataset Annotation with GroundingDINO and Segment Anything Model"
      ],
      "metadata": {
        "id": "a8-S5Ao3ImnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Grounding DINO and Segment Anything Model\n",
        "\n",
        "Our project will use two groundbreaking designs:\n",
        "- [Grounding DINO](https://arxiv.org/abs/2303.05499) for zero-shot object detection\n",
        "- [Segment Anything Model (SAM)](https://segment-anything.com/) for converting bounding boxes into segmentations. \n"
      ],
      "metadata": {
        "id": "s3VuH-MxY5ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To make it easier for us to manage datasets, images and models, we create a HOME constant.\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)"
      ],
      "metadata": {
        "id": "R_sWrwsuV0QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the GroundingDINO repository\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "# Install the necessary dependencies\n",
        "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "id": "mnlQNpMpwaP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install SAM\n",
        "%cd {HOME}\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "id": "vGwLQ7mBw9WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip uninstall -y supervision\n",
        "!pip install -q supervision==0.6.0\n",
        "\n",
        "import supervision as sv\n",
        "print(sv.__version__)\n",
        "\n",
        "!pip install -q roboflow\n",
        "\n",
        "from IPython.display import Image, clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "NxU6_aWCNSFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Grounding DINO Model Weights\n",
        "\n",
        "To run Grounding DINO we need two files - configuration and model weights. The configuration file is part of the [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) repository, which we have already cloned. The weights file, on the other hand, we need to download. We write the paths to both files to the `GROUNDING_DINO_CONFIG_PATH` and `GROUNDING_DINO_CHECKPOINT_PATH` variables and verify if the paths are correct and the files exist on disk."
      ],
      "metadata": {
        "id": "GGV-pHiDgZs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the GroundingDINO config file\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
      ],
      "metadata": {
        "id": "qtMIfEt4gdUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ],
      "metadata": {
        "id": "p-Eoargcg3Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the GroundingDINO checkpoint\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "BVkviPClh0Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Segment Anything Model (SAM) Weights\n",
        "\n",
        "As with Grounding DINO, in order to run SAM we need a weights file, which we must first download. We write the path to local weight file to `SAM_CHECKPOINT_PATH` variable and verify if the path is correct and the file exist on disk."
      ],
      "metadata": {
        "id": "hJqh1YOwdahU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "metadata": {
        "id": "XUEOO4bXdFJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the SAM checkpoint\n",
        "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "id": "ZsNiAQqTdM2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load models"
      ],
      "metadata": {
        "id": "ByAcxA43a484"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xJzLz7oUZ0e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Grounding DINO Model"
      ],
      "metadata": {
        "id": "98JPP_icc5hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import Model\n",
        "\n",
        "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"
      ],
      "metadata": {
        "id": "odl3HAdWc5Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Segment Anything Model (SAM)"
      ],
      "metadata": {
        "id": "DBTCh84cczrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "SAM_ENCODER_VERSION = \"vit_h\"\n",
        "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
        "sam_predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "-PBgnrmgcLSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Example Data"
      ],
      "metadata": {
        "id": "9GaaZzF0jseo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All images are stored in data/\n",
        "f\"{HOME}/data\"\n",
        "%cd {HOME}\n",
        "!mkdir {HOME}/data\n",
        "%cd {HOME}/data"
      ],
      "metadata": {
        "id": "XB-oJV2Gj6pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Kaggle API\n",
        "!pip install kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "AJzXMRbZW6G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Kaggle API credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json # Read and write permissions\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "HhDAE1-UXBjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d mostafaabla/garbage-classification\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip garbage-classification.zip\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "G8FRxVePXC2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 classes of images (garbages)\n",
        "!ls \"/content/data/garbage_classification/\""
      ],
      "metadata": {
        "id": "Pw0wbUWuXQk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9978b6-ccc0-496c-db5d-d676251998ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "battery     brown-glass  clothes      metal  plastic  trash\n",
            "biological  cardboard\t green-glass  paper  shoes    white-glass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single Image Mask Auto Annotation\n",
        "\n",
        "Before we automatically annotate the entire dataset let's focus for a moment on a single image."
      ],
      "metadata": {
        "id": "1euQGJ3ZG32x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/garbage_classification/green-glass/green-glass446.jpg\"\n",
        "CLASSES = ['green-glass']\n",
        "BOX_TRESHOLD = 0.40\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "id": "gCtHvuKoGa5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Object Detection with Grounding DINO"
      ],
      "metadata": {
        "id": "nrLU7SWFdGcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** To get better Grounding DINO detection we will leveragae prompt engineering using `enhance_class_name` function defined below. \n",
        "\n",
        "Source: [Grounding DINO tutorial](https://blog.roboflow.com/grounding-dino-zero-shot-object-detection/)."
      ],
      "metadata": {
        "id": "8Ll-gRnqvyOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def enhance_class_name(class_names: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Enhances class names by appending 's' and prepending 'all' to each class name.\n",
        "    \n",
        "    Args:\n",
        "        class_names (List[str]): List of class names\n",
        "    \n",
        "    Returns:\n",
        "        List[str]: Enhanced class names\n",
        "    \n",
        "    \"\"\"\n",
        "    return [\n",
        "        f\"all {class_name}s\"\n",
        "        for class_name\n",
        "        in class_names\n",
        "    ]"
      ],
      "metadata": {
        "id": "9EdxE5d6Hv3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision as sv\n",
        "\n",
        "# load image\n",
        "image = cv2.imread(SOURCE_IMAGE_PATH)\n",
        "\n",
        "# detect objects\n",
        "detections = grounding_dino_model.predict_with_classes(\n",
        "    image=image,\n",
        "    classes=enhance_class_name(class_names=CLASSES),\n",
        "    box_threshold=BOX_TRESHOLD,\n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "# get the index of the 'green-glass' class\n",
        "green_glass_id = CLASSES.index('green-glass')\n",
        "print(green_glass_id)\n",
        "\n",
        "# FORCE into xx class\n",
        "print(detections.class_id)\n",
        "print(detections)\n",
        "\n",
        "new_class_ids = []\n",
        "for cid in detections.class_id:\n",
        "    # assign the 'green-glass' class index if class_id is None\n",
        "    if cid is None:\n",
        "        new_class_id = green_glass_id\n",
        "    else:\n",
        "      new_class_id = cid\n",
        "    new_class_ids.append(new_class_id)\n",
        "\n",
        "# Assign the new class IDs to detections.class_id\n",
        "detections.class_id = new_class_ids\n",
        "\n",
        "print(detections.class_id)\n",
        "print(detections)\n",
        "\n",
        "# annotate image with detections\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "labels = [\n",
        "    f\"{CLASSES[class_id if class_id is not None else green_glass_id]} {confidence:0.2f}\" \n",
        "    for _, _, confidence, class_id, _ \n",
        "    in detections\n",
        "]\n",
        "\n",
        "annotated_frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "# Plot the annotated image\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_frame, (10, 10))"
      ],
      "metadata": {
        "id": "LoD2bIptG-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting SAM with detected boxes"
      ],
      "metadata": {
        "id": "NDnK-8N3eFbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from segment_anything import SamPredictor\n",
        "\n",
        "\n",
        "def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Segment objects in the image based on bounding boxes using the given SamPredictor.\n",
        "\n",
        "    Args:\n",
        "        sam_predictor (SamPredictor): SamPredictor instance for segmentation\n",
        "        image (np.ndarray): Input image for segmentation\n",
        "        xyxy (np.ndarray): Bounding boxes in the format [x1, y1, x2, y2]\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of segmented masks\n",
        "\n",
        "    \"\"\"\n",
        "    sam_predictor.set_image(image)\n",
        "    result_masks = []\n",
        "    for box in xyxy:\n",
        "        masks, scores, logits = sam_predictor.predict(\n",
        "            box=box,\n",
        "            multimask_output=True\n",
        "        )\n",
        "        index = np.argmax(scores) # Get the index of the highest score\n",
        "        result_masks.append(masks[index]) # Append the segmented mask with the highest score\n",
        "    return np.array(result_masks) # Return the segmented masks as an array"
      ],
      "metadata": {
        "id": "1ZX6WsB9NMUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# convert detections to masks\n",
        "detections.mask = segment(\n",
        "    sam_predictor=sam_predictor,\n",
        "    image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB), # Convert the image to RGB color space\n",
        "    xyxy=detections.xyxy\n",
        ")\n",
        "\n",
        "# annotate image with detections\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "green_glass_id = CLASSES.index('green-glass')\n",
        "labels = [\n",
        "    f\"{CLASSES[class_id if class_id is not None else green_glass_id]} {confidence:0.2f}\" \n",
        "    for _, _, confidence, class_id, _ \n",
        "    in detections\n",
        "]\n",
        "\n",
        "# Annotate the image with masks\n",
        "annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "\n",
        "# Annotate the image with bounding boxes and labels\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "# Plot the annotated image\n",
        "%matplotlib inline\n",
        "sv.plot_image(annotated_image, (16, 16))"
      ],
      "metadata": {
        "id": "wy3UjxMHclSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Dataset Mask Auto Annotation"
      ],
      "metadata": {
        "id": "AmFMQoEffdvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "IMAGES_EXTENSIONS = ['jpg', 'jpeg', 'png']\n",
        "\n",
        "IMAGES_DIRECTORY = os.path.join(HOME, 'data/garbage_classification/clothes')\n",
        "CLASSES = ['clothes']\n",
        "BOX_TRESHOLD = 0.40\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "IMAGES_DIRECTORY"
      ],
      "metadata": {
        "id": "g-RAp9Vofo2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract labels from images"
      ],
      "metadata": {
        "id": "TsqhBBWVuSTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "images = {}\n",
        "annotations = {}\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(\n",
        "    directory=IMAGES_DIRECTORY, \n",
        "    extensions=IMAGES_EXTENSIONS)\n",
        "\n",
        "\n",
        "# Randomly down-sample 1000 image paths\n",
        "image_paths_sample = random.sample(image_paths, 1000) # Toggle this!\n",
        "\n",
        "green_glass_id = CLASSES.index('clothes') # Toggle this!\n",
        "\n",
        "for image_path in tqdm(image_paths_sample):\n",
        "    image_name = image_path.name\n",
        "    image_path = str(image_path)\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    detections = grounding_dino_model.predict_with_classes(\n",
        "        image=image,\n",
        "        classes=enhance_class_name(class_names=CLASSES),\n",
        "        box_threshold=BOX_TRESHOLD,\n",
        "        text_threshold=TEXT_TRESHOLD\n",
        "    )\n",
        "    \n",
        "    # Replace all the None in class_id!!!\n",
        "    new_class_ids = []\n",
        "    for cid in detections.class_id:\n",
        "        if cid is None:\n",
        "            new_class_id = green_glass_id # Toggle this!\n",
        "        else:\n",
        "          new_class_id = cid\n",
        "        new_class_ids.append(new_class_id)\n",
        "    detections.class_id = new_class_ids\n",
        "    # Replace all the None in class_id!!!\n",
        "\n",
        "    detections.mask = segment(\n",
        "        sam_predictor=sam_predictor,\n",
        "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        xyxy=detections.xyxy\n",
        "    )\n",
        "    \n",
        "    images[image_name] = image\n",
        "    annotations[image_name] = detections"
      ],
      "metadata": {
        "id": "N-F8nruxNbcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save labels in Pascal VOC XML\n",
        "\n",
        "Before uploading our annotations to Roboflow, we must first save them to our hard drive. To do this, we will use one of the latest `supervision` features (available with the `0.6.0` update ðŸ”¥) - [dataset save](https://roboflow.github.io/supervision/dataset/core/)."
      ],
      "metadata": {
        "id": "traaYMeAuara"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANNOTATIONS_DIRECTORY = os.path.join(HOME, 'annotations/clothes')\n",
        "\n",
        "MIN_IMAGE_AREA_PERCENTAGE = 0.002\n",
        "MAX_IMAGE_AREA_PERCENTAGE = 0.80\n",
        "APPROXIMATION_PERCENTAGE = 0.75"
      ],
      "metadata": {
        "id": "OELQUXECX9sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.Dataset(\n",
        "    classes=CLASSES,\n",
        "    images=images,\n",
        "    annotations=annotations\n",
        ").as_pascal_voc(\n",
        "    annotations_directory_path=ANNOTATIONS_DIRECTORY,\n",
        "    min_image_area_percentage=MIN_IMAGE_AREA_PERCENTAGE,\n",
        "    max_image_area_percentage=MAX_IMAGE_AREA_PERCENTAGE,\n",
        "    approximation_percentage=APPROXIMATION_PERCENTAGE\n",
        ")"
      ],
      "metadata": {
        "id": "IdRAslKSmzJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save permanently to my Drive!"
      ],
      "metadata": {
        "id": "elpDzk8Sspsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J3Px4fVYDV7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload annotations to Roboflow\n",
        "\n",
        "Now we are ready to upload our annotations to Roboflow using the API."
      ],
      "metadata": {
        "id": "ghegsbVLvNhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = \"auto-generated-dataset-CSE493-clothes\"\n",
        "PROJECT_DESCRIPTION = \"auto-generated-dataset-CSE493-clothes\""
      ],
      "metadata": {
        "id": "FFJ0TdiuvTH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import roboflow\n",
        "from roboflow import Roboflow\n",
        "\n",
        "roboflow.login()\n",
        "\n",
        "workspace = Roboflow().workspace()\n",
        "new_project = workspace.create_project(\n",
        "    project_name=PROJECT_NAME,\n",
        "    project_license=\"MIT\",\n",
        "    project_type=\"instance-segmentation\", \n",
        "    annotation=PROJECT_DESCRIPTION)"
      ],
      "metadata": {
        "id": "3rCOMnT8vRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Iterate over the sampled image paths\n",
        "for image_path in tqdm(image_paths_sample):\n",
        "    image_name = image_path.name\n",
        "    annotation_name = f\"{image_path.stem}.xml\"\n",
        "    image_path = str(image_path)\n",
        "    annotation_path = os.path.join(ANNOTATIONS_DIRECTORY, annotation_name)\n",
        "\n",
        "    # Upload the image and annotation to the new project\n",
        "    new_project.upload(\n",
        "        image_path=image_path, \n",
        "        annotation_path=annotation_path, \n",
        "        split=\"train\", \n",
        "        is_prediction=True, \n",
        "        overwrite=True, \n",
        "        tag_names=[\"auto-annotated-with-grounded-sam\"],\n",
        "        batch_name=\"auto-annotated-with-grounded-sam\"\n",
        "    )"
      ],
      "metadata": {
        "id": "E9sb88IZvomH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}